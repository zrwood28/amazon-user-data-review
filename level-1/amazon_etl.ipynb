{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WosHdRWdmpw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.2.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "spark_version = 'spark-3.2.2'\n",
        "\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xISV-9eR6Yah"
      },
      "outputs": [],
      "source": [
        "# Install the PostgreSQL driver in our Colab environment\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEQFTWiV6g2h"
      },
      "outputs": [],
      "source": [
        "# Establish a Spark session and add the Postgres driver to the filepath\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qik6uGSV9BWa"
      },
      "outputs": [],
      "source": [
        "# Read the first CSV file from an S3 bucket\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.1/22-big-data/day_3/user_data.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "user_data_df = spark.read.csv(SparkFiles.get(\"user_data.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "# Show DataFrame\n",
        "user_data_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzqGVnoNd-d8"
      },
      "outputs": [],
      "source": [
        "# Read the next CSV file from an S3 bucket\n",
        "url = 'https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.1/22-big-data/day_3/user_payment.csv'\n",
        "spark.sparkContext.addFile(url)\n",
        "user_payment_df = spark.read.csv(SparkFiles.get(\"user_payment.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "# Rename the id column to billing_id\n",
        "user_payment_df = user_payment_df.withColumnRenamed(\"id\", \"billing_id\")\n",
        "\n",
        "# Show DataFrame\n",
        "user_payment_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoN3Gyb4fkhC"
      },
      "outputs": [],
      "source": [
        "# Join the two DataFrames \n",
        "joined_df= user_data_df.join(user_payment_df, on=\"username\", how=\"inner\")\n",
        "joined_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtDndjrdlh83"
      },
      "outputs": [],
      "source": [
        "# Drop null values\n",
        "dropna_df = joined_df.dropna(how='any')\n",
        "dropna_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjEBAydllvCg"
      },
      "outputs": [],
      "source": [
        "# Load a function that allows us to select columns\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Filter for only columns with active users\n",
        "cleaned_df = dropna_df.filter(col(\"active_user\") == True)\n",
        "cleaned_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVzKY3EjmUoj"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe for the active_user table in our database\n",
        "clean_user_df = cleaned_df.select([\"billing_id\", \"first_name\", \"last_name\", \"username\"])\n",
        "clean_user_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6mUzCILmj9X"
      },
      "outputs": [],
      "source": [
        "# Create a billing dataframe for the billing_info table in our database\n",
        "clean_billing_df = cleaned_df.select([\"billing_id\", \"street_address\", \"state\", \"username\"])\n",
        "clean_billing_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVcL8Ub7mmaY"
      },
      "outputs": [],
      "source": [
        "# Create a payment dataframe for the payment_info table in our database\n",
        "clean_payment_df = cleaned_df.select([\"billing_id\", \"cc_encrypted\"])\n",
        "clean_payment_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQgPpNnVnRwh"
      },
      "source": [
        "Postgres Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "985Vl1dtmpVU"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: Replace each of these parameters with your own values for your AWS RDS instance\n",
        "my_aws_endpoint = 'mypostgresdb.czwtrbwfkobv.us-east-2.rds.amazonaws.com' # This is my value; please replace with your own\n",
        "my_aws_port_number = '5432' # Your value is likely the same, but please double check\n",
        "my_aws_database_name = 'my_database' # This is my value; please replace with your own\n",
        "my_aws_username = 'postgres' # Your value is likely the same, but please double check\n",
        "my_aws_password = '<password>' # This is my value; please replace with your own\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the connection string\n",
        "jdbc_url=f'jdbc:postgresql://{my_aws_endpoint}:{my_aws_port_number}/{my_aws_database_name}'\n",
        "\n",
        "# Set up the configuration parameters\n",
        "config = {\"user\": f'{my_aws_username}', \n",
        "          \"password\": f'{my_aws_password}', \n",
        "          \"driver\":\"org.postgresql.Driver\"}\n",
        "\n",
        "# Choose to overwrite the existing data. Note that 'append' is probably a smarter choice\n",
        "# in those situations where the PostgreSQL schema automatically generates the primary key. \n",
        "#  But 'overwrite' works best for this little demo. \n",
        "mode = 'overwrite' \n"
      ],
      "metadata": {
        "id": "aemKXQGz73xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yyZgvcUnaNN"
      },
      "outputs": [],
      "source": [
        "# Write the dataframe to the appropriate table in your PostgreSQL RDS\n",
        "\n",
        "clean_user_df.write.jdbc(url=jdbc_url, table='active_user', mode=mode, properties=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rsTf2kmna3V"
      },
      "outputs": [],
      "source": [
        "# Write the dataframe to the appropriate table in your PostgreSQL RDS\n",
        "\n",
        "clean_billing_df.write.jdbc(url=jdbc_url, table='billing_info', mode=mode, properties=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q6Bm3kAncqx"
      },
      "outputs": [],
      "source": [
        "# Write the dataframe to the appropriate table in your PostgreSQL RDS\n",
        "\n",
        "clean_payment_df.write.jdbc(url=jdbc_url, table='payment_info', mode=mode, properties=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx3hMnlAD5FA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}